{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZq_yZRcbxdI"
   },
   "source": [
    "AI Suite is a light wrapper to provide a unified interface between LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mt8kgFHXMvv",
    "outputId": "b56619e8-0dd8-4850-d3b2-1f1169672aab"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aisuite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Install AI Suite\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install -e \"/User/hubertus/Projects/aisuite/\" # \"aisuite[all]\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maisuite\u001b[39;00m \u001b[38;5;66;03m# [all]'\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aisuite'"
     ]
    }
   ],
   "source": [
    "# Install AI Suite\n",
    "#!pip install -e \"/User/hubertus/Projects/aisuite/\" # \"aisuite[all]\"\n",
    "import aisuite # [all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwFlLByRbWKi"
   },
   "source": [
    "### Custom Pretty Printing Function\n",
    "In this section, we define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-Wf7j6abbQmw"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "# Set a custom width for pretty-printing\n",
    "def pprint(data, width=80):\n",
    "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
    "    pp(data, width=width)# List of model identifiers to query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cce1aLBvctaL"
   },
   "source": [
    "### Setting Up API Keys\n",
    "\n",
    "Here we will securely set our API keys as environment variables. This is helpful because we don’t want to hardcode sensitive information (like API keys) directly into our code. By using environment variables, we can keep our credentials secure while still allowing our program to access them. Normally we would use a .env file to store our passwords to our enviroments, but since we are going to be working in colab we will do things a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsK7GrHyV-c4",
    "outputId": "35fef9dc-e226-4e9d-e6c7-a597882b74f9"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GROQ API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2mhu-VbSWfF"
   },
   "source": [
    "### Creating a Simple Chat Interaction with an AI Language Model\n",
    "This code initiates a chat interaction with a language model (specifically Groq’s LLaMA 3.2), where the model responds to the user's input. We use the aisuite library to communicate with the model and retrieve the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBEOEq99eGjR",
    "outputId": "446fdba3-9072-4470-b3b8-627717013604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I assist you?\n"
     ]
    }
   ],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "# Initialize the AI client for accessing the language model\n",
    "client = ai.Client()\n",
    "\n",
    "# Define a conversation with a system message and a user message\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
    "    {\"role\": \"user\", \"content\": 'Hi'},\n",
    "]\n",
    "\n",
    "# Request a response from the model\n",
    "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
    "\n",
    "# Print the model's response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJSahowjiJBE"
   },
   "source": [
    "### Defining a Function to Interact with the Language Model\n",
    "\n",
    "This function, ask, streamlines the process of sending a user message to a language model and retrieving a response. It encapsulates the logic required to set up the conversation and can be reused throughout the notebook for different queries. It will not perserve any history or any continuing conversation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n8DK8_RqqXFH"
   },
   "outputs": [],
   "source": [
    "def ask(message, sys_message=\"You are a helpful agent.\",\n",
    "         model=\"groq:llama-3.2-3b-preview\"):\n",
    "    # Initialize the AI client for accessing the language model\n",
    "    client = ai.Client()\n",
    "\n",
    "    # Construct the messages list for the chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_message},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "\n",
    "    # Send the messages to the model and get the response\n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    # Return the content of the model's response\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FGcqY4lBjtFj",
    "outputId": "0520933a-8f2f-4185-a8a2-c591283482a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello. The capital of Japan is Tokyo.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"Hi. what is capital of Japan?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpeW6Pj6j_6H"
   },
   "source": [
    "The real value of AI Suite is the ablity to run a variety of different models.  Let's first set up a collection of different API keys which we can try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_kJlkGfj_NG",
    "outputId": "d45074c6-bbc6-4214-df0c-6d162a176f21"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getpass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter your OPENAI API key: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANTHROPIC_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter your ANTHROPIC API key: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getpass' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI API key: ')\n",
    "os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your ANTHROPIC API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfPtlJlbTY6X"
   },
   "source": [
    "###Confirm each model is using a different provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHVESCGJuWWg",
    "outputId": "3102b43a-e754-4288-ec1d-9777791f25b6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is your creator?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ask(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho is your creator?\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manthropic:claude-3-5-sonnet-20240620\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ask(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho is your creator?\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai:gpt-4o\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mask\u001b[0;34m(message, sys_message, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask\u001b[39m(message, sys_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful agent.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m          model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroq:llama-3.2-3b-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Initialize the AI client for accessing the language model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mai\u001b[49m\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Construct the messages list for the chat\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sys_message},\n\u001b[1;32m      9\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}\n\u001b[1;32m     10\u001b[0m     ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ai' is not defined"
     ]
    }
   ],
   "source": [
    "print(ask(\"Who is your creator?\"))\n",
    "print(ask('Who is your creator?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
    "print(ask('Who is your creator?', model='openai:gpt-4o'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWBL4D2H2B_9"
   },
   "source": [
    "### Querying Multiple AI Models for a Common Question\n",
    "In this section, we will query several different versions of the LLaMA language model to get varied responses to the same question. This approach allows us to compare how different models handle the same prompt, providing insights into their performance and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_gg-sgYuoOb",
    "outputId": "d1c582ba-3471-4b0e-b9ca-317df8a1c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('llama-3.1-8b-instant: \\n'\n",
      " ' The origins of Artificial Intelligence (AI) date back to the 1956 Dartmouth '\n",
      " 'Summer Research Project on Artificial Intelligence, where a group of '\n",
      " 'computer scientists, led by John McCarthy, Marvin Minsky, Nathaniel '\n",
      " 'Rochester, and Claude Shannon, coined the term and laid the foundation for '\n",
      " 'the development of AI as a distinct field of study. ')\n",
      "('llama-3.2-1b-preview: \\n'\n",
      " ' The origins of Artificial Intelligence (AI) date back to the mid-20th '\n",
      " 'century, when the first computer programs, which mimicked human-like '\n",
      " 'intelligence through algorithms and rule-based systems, were developed by '\n",
      " 'renowned mathematicians and computer scientists, including Alan Turing, '\n",
      " 'Marvin Minsky, and John McCarthy in the 1950s. ')\n",
      "('llama-3.2-3b-preview: \\n'\n",
      " ' The origins of Artificial Intelligence (AI) date back to the 1950s, with '\n",
      " 'the Dartmouth Summer Research Project on Artificial Intelligence, led by '\n",
      " 'computer scientists John McCarthy, Marvin Minsky, and Nathaniel Rochester, '\n",
      " 'marking the birth of AI as a formal field of research. ')\n",
      "('llama3-70b-8192: \\n'\n",
      " ' The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
      " 'when computer scientist Alan Turing proposed the Turing Test, a method for '\n",
      " 'determining whether a machine could exhibit intelligent behavior equivalent '\n",
      " 'to, or indistinguishable from, that of a human. ')\n",
      "('llama3-8b-8192: \\n'\n",
      " ' The origins of Artificial Intelligence (AI) can be traced back to the '\n",
      " '1950s, when computer scientists DARPA funded the development of the first AI '\n",
      " 'programs, such as the Logical Theorist, which aimed to simulate human '\n",
      " 'problem-solving abilities and learn from experience. ')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = [\n",
    "    'llama-3.1-8b-instant',\n",
    "    'llama-3.2-1b-preview',\n",
    "    'llama-3.2-3b-preview',\n",
    "    'llama3-70b-8192',\n",
    "    'llama3-8b-8192'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each model\n",
    "ret = []\n",
    "\n",
    "# Loop through each model and get a response for the specified question\n",
    "for x in models:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'groq:{x}'))\n",
    "\n",
    "# Print the model's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(models[idx] + ': \\n ' + x + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8pnJPdD2NL0"
   },
   "source": [
    "### Querying Different AI Providers for a Common Question\n",
    "In this section, we will query multiple AI models from different providers to get varied responses to the same question regarding the origins of AI. This comparison allows us to observe how different models from different architectures respond to the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "j4TqhC5J1YIG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4a50e300-0a7a-4562-8a34-f31c4b9072d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('groq:llama3-70b-8192: \\n'\n",
      " 'The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
      " 'when computer scientists like Alan Turing, Marvin Minsky, and John McCarthy '\n",
      " 'began exploring ways to create machines that could think and learn like '\n",
      " 'humans, leading to the development of the first AI programs and '\n",
      " 'algorithms. \\n'\n",
      " '\\n')\n",
      "('openai:gpt-4o: \\n'\n",
      " 'The origins of AI trace back to the mid-20th century, when pioneers like '\n",
      " 'Alan Turing and John McCarthy began exploring the possibility of creating '\n",
      " 'machines that could simulate human intelligence through computational '\n",
      " 'processes. \\n'\n",
      " '\\n')\n",
      "('anthropic:claude-3-5-sonnet-20240620: \\n'\n",
      " 'The origins of AI can be traced back to the 1950s when computer scientists '\n",
      " 'began exploring the concept of creating machines that could simulate human '\n",
      " 'intelligence and problem-solving abilities. \\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "# List of AI model providers to query\n",
    "providers = [\n",
    "    'groq:llama3-70b-8192',\n",
    "    'openai:gpt-4o',\n",
    "    'anthropic:claude-3-5-sonnet-20240620'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold the responses from each provider\n",
    "ret = []\n",
    "\n",
    "# Loop through each provider and get a response for the specified question\n",
    "for x in providers:\n",
    "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
    "\n",
    "# Print the provider's name and its corresponding response\n",
    "for idx, x in enumerate(ret):\n",
    "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgPCC0y_U4WG"
   },
   "source": [
    "### Generating and Evaluating Questions with AI Models\n",
    "In this section, we will randomly generate questions using a language model and then have two other models provide answers to those questions. The user will then evaluate which answer is better, allowing for a comparative analysis of responses from different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "fMx-TfLk09ft",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "56153c03-a1e6-4b72-fd16-b36197ccb5ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Original text:\\n'\n",
      " \"  Here's a short question suitable for asking an LLM:\\n\"\n",
      " '\\n'\n",
      " 'What are the potential benefits and risks of artificial intelligence in '\n",
      " 'healthcare?\\n'\n",
      " '\\n')\n",
      "('Option 1 text:\\n'\n",
      " '  **Benefits:**\\n'\n",
      " '1. Improved diagnostics and personalized treatment plans.\\n'\n",
      " '2. Increased efficiency in administrative tasks.\\n'\n",
      " '3. Faster drug discovery and development.\\n'\n",
      " '4. Enhanced patient monitoring and support.\\n'\n",
      " '\\n'\n",
      " '**Risks:**\\n'\n",
      " '1. Privacy and data security concerns.\\n'\n",
      " '2. Potential biases in AI algorithms.\\n'\n",
      " '3. Over-reliance on AI systems by healthcare professionals.\\n'\n",
      " '4. Ethical and accountability issues in decision-making.\\n'\n",
      " '\\n')\n",
      "('Option 2 text:\\n'\n",
      " '  The potential benefits of artificial intelligence (AI) in healthcare '\n",
      " 'include:\\n'\n",
      " '\\n'\n",
      " '* Improved diagnosis accuracy and speed\\n'\n",
      " '* Enhanced patient outcomes through personalized medicine\\n'\n",
      " '* Increased efficiency and reduced costs through automation\\n'\n",
      " '* Better disease prevention and detection\\n'\n",
      " '* Enhanced research capabilities and new treatment discoveries\\n'\n",
      " '\\n'\n",
      " 'However, there are also potential risks, such as:\\n'\n",
      " '\\n'\n",
      " '* Bias in AI decision-making due to flawed data or algorithms\\n'\n",
      " '* Job displacement of healthcare professionals\\n'\n",
      " '* Cybersecurity risks to patient data\\n'\n",
      " '* Dependence on technology leading to deskilling of healthcare workers\\n'\n",
      " '* Unintended consequences of AI-driven decision-making that may not align '\n",
      " 'with human values.\\n'\n",
      " '\\n'\n",
      " 'These benefits and risks highlight the need for responsible development, '\n",
      " 'deployment, and oversight of AI in healthcare.\\n'\n",
      " '\\n')\n",
      "Which is best 1 or 2. 3 if indistinguishable: 3\n",
      "('Original text:\\n'\n",
      " '  What are the potential applications of large language models in '\n",
      " 'healthcare?\\n'\n",
      " '\\n')\n",
      "('Option 1 text:\\n'\n",
      " '  Large language models have numerous potential applications in healthcare, '\n",
      " 'including:\\n'\n",
      " '\\n'\n",
      " '1. **Clinical Decision Support**: Providing doctors with accurate diagnoses, '\n",
      " 'treatment options, and medication recommendations.\\n'\n",
      " '2. **Medical Text Analysis**: Analyzing large amounts of medical literature, '\n",
      " 'patient records, and clinical notes to identify patterns and insights.\\n'\n",
      " '3. **Patient Engagement**: Generating personalized health summaries, '\n",
      " 'communicating medical information in simple language, and facilitating '\n",
      " 'patient-provider communication.\\n'\n",
      " '4. **Disease Surveillance**: Monitoring social media and online platforms '\n",
      " 'for disease outbreaks and tracking epidemiological trends.\\n'\n",
      " '5. **Medical Writing Assistance**: Assisting healthcare professionals in '\n",
      " 'generating medical reports, discharge summaries, and other documents.\\n'\n",
      " '6. **Chatbots and Virtual Assistants**: Offering patients timely support and '\n",
      " 'answers to medical queries.\\n'\n",
      " '7. **Research and Development**: Accelerating biomedical research by '\n",
      " 'analyzing large datasets, identifying research gaps, and suggesting '\n",
      " 'potential areas of investigation.\\n'\n",
      " '\\n'\n",
      " 'These applications have the potential to improve healthcare outcomes, reduce '\n",
      " 'costs, and enhance patient experiences.\\n'\n",
      " '\\n')\n",
      "('Option 2 text:\\n'\n",
      " '  Large language models in healthcare could potentially be used for:\\n'\n",
      " '\\n'\n",
      " '1. Clinical decision support\\n'\n",
      " '2. Medical literature analysis and summarization\\n'\n",
      " '3. Patient triage and symptom checking\\n'\n",
      " '4. Medical education and training\\n'\n",
      " '5. Automated medical coding and documentation\\n'\n",
      " '6. Drug discovery and development\\n'\n",
      " '7. Personalized treatment recommendations\\n'\n",
      " '8. Health-related chatbots for patient engagement\\n'\n",
      " '9. Medical research and hypothesis generation\\n'\n",
      " '10. Natural language processing of electronic health records\\n'\n",
      " '\\n'\n",
      " 'These applications could help improve efficiency, accuracy, and '\n",
      " 'accessibility in various aspects of healthcare.\\n'\n",
      " '\\n')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d17783dc1f7c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store the provider names and the user's choice of the best answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Which is best 1 or 2. 3 if indistinguishable: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Initialize a list to store the best responses\n",
    "best = []\n",
    "\n",
    "# Loop to generate and evaluate questions\n",
    "for _ in range(20):\n",
    "    # Shuffle the providers list to randomly select models for each iteration\n",
    "    random.shuffle(providers)\n",
    "\n",
    "    # Generate a question using the first provider\n",
    "    question = ask('Please generate a short question that is suitable for asking an LLM.', model=providers[0])\n",
    "\n",
    "    # Get answers from the second and third providers\n",
    "    answer_1 = ask('Please give a short answer to this question: ' + question, model=providers[1])\n",
    "    answer_2 = ask('Please give a short answer to this question: ' + question, model=providers[2])\n",
    "\n",
    "    # Print the generated question and the two answers\n",
    "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
    "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
    "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
    "\n",
    "    # Store the provider names and the user's choice of the best answer\n",
    "    best.append(str(providers) + ', ' + input(\"Which is best 1 or 2. 3 if indistinguishable: \"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
